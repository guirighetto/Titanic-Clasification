{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Description: Method of classification using QR code and convolutional neural network\n",
    "- Author: Guilherme Righetto\n",
    "- Sense: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import qrcode\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import PIL.Image\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "import torch.optim.lr_scheduler\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "from lib import pytorch_trainer as ptt\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU available:', use_gpu)\n",
    "\n",
    "CREATE_QR = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Munging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Data/train.csv\", sep=\",\")\n",
    "df_test = pd.read_csv(\"Data/test.csv\", sep=\",\")\n",
    "\n",
    "df_train[\"_data\"] = \"train\"\n",
    "df_test[\"_data\"] = \"test\"\n",
    "\n",
    "list_test = df_test[\"PassengerId\"].tolist()\n",
    "\n",
    "df_data = pd.concat([df_train, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data.loc[df_data[\"Sex\"] == 'male', \"Sex\"] = 0\n",
    "df_data.loc[df_data[\"Sex\"] == 'female', \"Sex\"] = 1\n",
    "df_data[\"Sex\"] = df_data[\"Sex\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data[\"Embarked\"].fillna(\"S\", inplace=True)\n",
    "\n",
    "df_data.loc[df_data[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "df_data.loc[df_data[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "df_data.loc[df_data[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "df_data[\"Embarked\"] = df_data[\"Embarked\"].astype(int)\n",
    "\n",
    "df_tmp = pd.get_dummies(df_data[\"Embarked\"], prefix=\"Embarked\")\n",
    "del df_data[\"Embarked\"]\n",
    "df_data = pd.concat([df_data, df_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data[\"Cabin\"].fillna(\"N\", inplace=True)\n",
    "df_data[\"Cabin\"] = df_data[\"Cabin\"].str.replace(r'\\d+',\"\")\n",
    "df_data[\"Cabin\"] = df_data[\"Cabin\"].str.replace(r' .*',\"\")\n",
    "\n",
    "df_data.loc[df_data[\"Cabin\"] == \"A\", \"Cabin\"] = 0\n",
    "df_data.loc[df_data[\"Cabin\"] == \"B\", \"Cabin\"] = 1\n",
    "df_data.loc[df_data[\"Cabin\"] == \"C\", \"Cabin\"] = 3\n",
    "df_data.loc[df_data[\"Cabin\"] == \"D\", \"Cabin\"] = 9\n",
    "df_data.loc[df_data[\"Cabin\"] == \"E\", \"Cabin\"] = 15\n",
    "df_data.loc[df_data[\"Cabin\"] == \"F\", \"Cabin\"] = 31\n",
    "df_data.loc[df_data[\"Cabin\"] == \"G\", \"Cabin\"] = 63\n",
    "df_data.loc[df_data[\"Cabin\"] == \"T\", \"Cabin\"] = 127\n",
    "df_data.loc[df_data[\"Cabin\"] == \"N\", \"Cabin\"] = 255\n",
    "df_data[\"Cabin\"] = df_data[\"Cabin\"].astype(int)\n",
    "\n",
    "df_tmp = pd.get_dummies(df_data[\"Cabin\"], prefix=\"Cabin\")\n",
    "del df_data[\"Cabin\"]\n",
    "df_data = pd.concat([df_data, df_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data[\"Ticket\"] = df_data[\"Ticket\"].str.replace(r'\\W',\"\")\n",
    "df_data[\"Ticket\"] = df_data[\"Ticket\"].str.replace(r'\\d',\"\")\n",
    "df_data.loc[df_data[\"Ticket\"] == \"\", \"Ticket\"] = \"N\"\n",
    "df_data[\"Ticket\"] = df_data[\"Ticket\"].astype('category').cat.codes\n",
    "\n",
    "df_tmp = pd.get_dummies(df_data[\"Ticket\"], prefix=\"Ticket\")\n",
    "del df_data[\"Ticket\"]\n",
    "df_data = pd.concat([df_data, df_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data[\"Name\"] = df_data[\"Name\"].str.replace(r'.*Mr.*',\"Mrs\")\n",
    "df_data[\"Name\"] = df_data[\"Name\"].str.replace(r'.*Miss.*',\"Miss\")\n",
    "df_data[\"Name\"] = df_data[\"Name\"].str.replace(r'.*Master.*',\"Master\")\n",
    "df_data[\"Name\"] = df_data[\"Name\"].str.replace(r'.*Dr.*',\"Dr\")\n",
    "df_data[\"Name\"] = df_data[\"Name\"].str.replace(r'.*Mlle.*|.*Mme.*',\"Mlle\")\n",
    "df_data[\"Name\"] = df_data[\"Name\"].str.replace(r'.*Capt.*|.*Don.*|.*Major.*|.*Sir.*|.*Jonkheer.*',\"Sir\")\n",
    "df_data[\"Name\"] = df_data[\"Name\"].str.replace(r'.*Dona.*|.*Lady.*|.*the Countess.*',\"lady\")\n",
    "df_data[\"Name\"] = df_data[\"Name\"].str.replace(r'.* .*',\"Others\")\n",
    "\n",
    "df_data.loc[df_data[\"Name\"] == \"Miss\", \"Name\"] = 127\n",
    "df_data.loc[df_data[\"Name\"] == \"Mlle\", \"Name\"] = 63\n",
    "df_data.loc[df_data[\"Name\"] == \"lady\", \"Name\"] = 31\n",
    "df_data.loc[df_data[\"Name\"] == \"Master\", \"Name\"] = 15\n",
    "df_data.loc[df_data[\"Name\"] == \"Sir\", \"Name\"] = 9\n",
    "df_data.loc[df_data[\"Name\"] == \"Dr\", \"Name\"] = 3\n",
    "df_data.loc[df_data[\"Name\"] == \"Mrs\", \"Name\"] = 1\n",
    "df_data.loc[df_data[\"Name\"] == \"Others\", \"Name\"] = 0\n",
    "df_data[\"Name\"] = df_data[\"Name\"].astype(int)\n",
    "\n",
    "df_tmp = pd.get_dummies(df_data[\"Name\"], prefix=\"Name\")\n",
    "del df_data[\"Name\"]\n",
    "df_data = pd.concat([df_data, df_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data[\"FamilySize\"] = df_data[\"SibSp\"] + df_data[\"Parch\"] + 1\n",
    "\n",
    "df_data[\"Age\"].fillna(int(df_data[\"Age\"].mean()), inplace=True)\n",
    "\n",
    "df_data[\"Children\"] = 0\n",
    "df_data.loc[df_data[\"Age\"] < 16, \"Children\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Cabin', 'Fare', 'Parch', 'PassengerId', 'Pclass', 'Sex',\n",
       "       'SibSp', 'Survived', '_data', 'Embarked_0', 'Embarked_1', 'Embarked_2',\n",
       "       'Ticket_0', 'Ticket_1', 'Ticket_2', 'Ticket_3', 'Ticket_4', 'Ticket_5',\n",
       "       'Ticket_6', 'Ticket_7', 'Ticket_8', 'Ticket_9', 'Ticket_10',\n",
       "       'Ticket_11', 'Ticket_12', 'Ticket_13', 'Ticket_14', 'Ticket_15',\n",
       "       'Ticket_16', 'Ticket_17', 'Ticket_18', 'Ticket_19', 'Ticket_20',\n",
       "       'Ticket_21', 'Ticket_22', 'Ticket_23', 'Ticket_24', 'Ticket_25',\n",
       "       'Ticket_26', 'Ticket_27', 'Ticket_28', 'Ticket_29', 'Ticket_30',\n",
       "       'Ticket_31', 'Ticket_32', 'Name_0', 'Name_1', 'Name_3', 'Name_9',\n",
       "       'Name_15', 'Name_31', 'Name_63', 'Name_127', 'FamilySize', 'Children'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_data.loc[df_data[\"_data\"] == \"train\"].copy()\n",
    "del df_train[\"_data\"]\n",
    "\n",
    "df_test = df_data.loc[df_data[\"_data\"] == \"test\"].copy()\n",
    "del df_test[\"_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train, df_validation = train_test_split(df_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting data to QR code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(CREATE_QR):\n",
    "    i=0\n",
    "    for line in df_train.iterrows():\n",
    "        qr = qrcode.QRCode(version=2, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=2, border=0)\n",
    "\n",
    "        data = line[1]\n",
    "\n",
    "        if(data[\"Survived\"] == 1.0):\n",
    "            del data[\"Survived\"]\n",
    "            qr.add_data(list(data.values))\n",
    "            qr.make(fit=True)\n",
    "            img = qr.make_image()\n",
    "            img.save(\"Data/Train/1_\" + str(i) + \".png\")\n",
    "        elif(data[\"Survived\"] == 0.0):\n",
    "            del data[\"Survived\"]\n",
    "            qr.add_data(list(data.values))\n",
    "            qr.make(fit=True)\n",
    "            img = qr.make_image()\n",
    "            img.save(\"Data/Train/0_\" + str(i) + \".png\")\n",
    "        i+=1\n",
    "    for line in df_validation.iterrows():\n",
    "        qr = qrcode.QRCode(version=2, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=2, border=0)\n",
    "\n",
    "        data = line[1]\n",
    "\n",
    "        if(data[\"Survived\"] == 1.0):\n",
    "            del data[\"Survived\"]\n",
    "            qr.add_data(list(data.values))\n",
    "            qr.make(fit=True)\n",
    "            img = qr.make_image()\n",
    "            img.save(\"Data/Validation/1_\" + str(i) + \".png\")\n",
    "        elif(data[\"Survived\"] == 0.0):\n",
    "            del data[\"Survived\"]\n",
    "            qr.add_data(list(data.values))\n",
    "            qr.make(fit=True)\n",
    "            img = qr.make_image()\n",
    "            img.save(\"Data/Validation/0_\" + str(i) + \".png\")\n",
    "        i+=1\n",
    "    for line in df_test.iterrows():\n",
    "        qr = qrcode.QRCode(version=2, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=2, border=0)\n",
    "\n",
    "        data = line[1]\n",
    "\n",
    "        del data[\"Survived\"]\n",
    "        qr.add_data(list(data.values))\n",
    "        qr.make(fit=True)\n",
    "        img = qr.make_image()\n",
    "        img.save(\"Data/Test/\" + str(i) + \".png\")\n",
    "        i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154, 154)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TitanicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, rootdir, train=True, transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        if self.train:\n",
    "            self.dirpath = os.path.join(rootdir, 'Train/')\n",
    "        else:\n",
    "            self.dirpath = os.path.join(rootdir, 'Validation/')\n",
    "\n",
    "        self.l_filepaths = [fp for fp in sorted(glob.glob(os.path.join(self.dirpath, '**'), recursive=True))\n",
    "                            if fp[-4:].lower() == '.png']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.l_filepaths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fp = self.l_filepaths[index]\n",
    "        out = int(os.path.basename(fp)[:1].lower() == '0')\n",
    "        inp = PIL.Image.open(fp).convert('L')\n",
    "        if self.transform is not None:\n",
    "            inp = self.transform(inp)\n",
    "        return inp, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 ,.,.) = \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0039  0.0627  ...   0.0627  0.0039  0.0000\n",
       "  0.0000  0.0627  1.0000  ...   1.0000  0.0627  0.0000\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0627  1.0000  ...   0.0000  0.9373  1.0000\n",
       "  0.0000  0.0039  0.0627  ...   0.0000  0.9373  1.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.9373  1.0000\n",
       "[torch.FloatTensor of size 1x150x150]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootdir = 'Data/'\n",
    "\n",
    "size_final = (150, 150)\n",
    "transf_comp_train = torchvision.transforms.Compose([torchvision.transforms.Scale(size=size_final),\n",
    "                                                    torchvision.transforms.ToTensor()])\n",
    "\n",
    "transf_comp_valid = torchvision.transforms.Compose([torchvision.transforms.Scale(size=size_final),\n",
    "                                                    torchvision.transforms.ToTensor()])\n",
    "\n",
    "dataset_train = TitanicDataset(rootdir, train=True, transform=transf_comp_train)\n",
    "dataset_valid = TitanicDataset(rootdir, train=False, transform=transf_comp_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=40, shuffle=True, num_workers=4)\n",
    "loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=40, shuffle=False, num_workers=4)\n",
    "\n",
    "dataloaders = {'train': loader_train,\n",
    "               'val': loader_valid\n",
    "              }\n",
    "\n",
    "print('Size - Train:', len(df_train))\n",
    "print('Size - Test:', len(df_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # Camadas convolucionais\n",
    "        self.conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=3, padding=1)),   \n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('max_pool1', nn.MaxPool2d(2)),\n",
    "#             ('drop1', nn.Dropout(p=0.5)),\n",
    "            \n",
    "            ('conv2', nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, padding=1)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('max_pool2', nn.MaxPool2d(2)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(in_channels=50, out_channels=30, kernel_size=3, padding=1)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('max_pool3', nn.MaxPool2d(2)),\n",
    "            ('drop3', nn.Dropout(p=0.5))\n",
    "        ]))\n",
    "        \n",
    "        # Camadas densas\n",
    "        self.dense_layer = nn.Sequential(OrderedDict([\n",
    "            ('dense1', nn.Linear(in_features=9720, out_features=450)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "#             ('dense2', nn.Linear(in_features=450, out_features=100)),\n",
    "#             ('relu2', nn.ReLU()),\n",
    "#             ('drop2', nn.Dropout(p=0.5)),\n",
    "            ('dense3', nn.Linear(in_features=450, out_features=2)),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(-1, 9720) \n",
    "        x = self.dense_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da saída convolucional: torch.Size([1, 30, 18, 18])\n",
      "Dimensão após a vetorização: torch.Size([1, 9720])\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "\n",
    "# input do mesmo tamanho que as imagens usadas\n",
    "example_input = Variable(torch.zeros(1, 1, 150, 150))\n",
    "\n",
    "# output da camada convolucional\n",
    "example_output = model.conv_layer(example_input)\n",
    "\n",
    "print('Dimensões da saída convolucional:', example_output.size())\n",
    "print('Dimensão após a vetorização:', example_output.view(1, -1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.1)\n",
    "\n",
    "trainer = ptt.DeepNetTrainer(\n",
    "    model=model, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer,\n",
    "    #lr_scheduler=0.01,#lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1),\n",
    "    callbacks = [ptt.AccuracyMetric(),\n",
    "                 ptt.PrintCallback(),\n",
    "                 ptt.ModelCheckpoint('Models/titanic-ori', reset=True, verbose=1)],\n",
    "    use_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 30 epochs\n",
      " 11:   2.1s   T: 0.63554 0.62919   V: 0.65145 0.59322 \n",
      " 12:   2.1s   T: 0.62565 0.63926   V: 0.69386 0.59322 \n",
      " 13:   2.1s   T: 0.62658 0.62919   V: 0.64053 0.60678 best\n",
      " 14:   2.1s   T: 0.63333 0.64094   V: 0.63486 0.61356 best\n",
      " 15:   2.1s   T: 0.63362 0.64430   V: 0.65754 0.59322 \n",
      " 16:   2.1s   T: 0.62419 0.64765   V: 0.62907 0.60678 best\n",
      " 17:   2.2s   T: 0.62004 0.65940   V: 0.62929 0.61356 \n",
      " 18:   2.1s   T: 0.61587 0.66275   V: 0.63434 0.61695 \n",
      " 19:   2.1s   T: 0.61022 0.67785   V: 0.62693 0.61356 best\n",
      " 20:   2.1s   T: 0.60907 0.70134   V: 0.64468 0.61695 \n",
      " 21:   2.1s   T: 0.60761 0.67617   V: 0.62449 0.62034 best\n",
      " 22:   2.1s   T: 0.60812 0.67450   V: 0.64472 0.61695 \n",
      " 23:   2.1s   T: 0.59602 0.66611   V: 0.66486 0.61695 \n",
      " 24:   2.1s   T: 0.59080 0.66946   V: 0.61987 0.63051 best\n",
      " 25:   2.1s   T: 0.59166 0.68960   V: 0.68772 0.61695 \n",
      " 26:   2.1s   T: 0.60129 0.68456   V: 0.61666 0.62373 best\n",
      " 27:   2.1s   T: 0.58454 0.69128   V: 0.61883 0.63390 \n",
      " 28:   2.1s   T: 0.57639 0.70973   V: 0.64652 0.60678 \n",
      " 29:   2.1s   T: 0.57345 0.70638   V: 0.63360 0.61356 \n",
      " 30:   2.1s   T: 0.56902 0.70638   V: 0.61030 0.68814 best\n",
      " 31:   2.2s   T: 0.57020 0.71141   V: 0.60552 0.69153 best\n",
      " 32:   2.1s   T: 0.55559 0.72819   V: 0.61182 0.68136 \n",
      " 33:   2.1s   T: 0.56216 0.70302   V: 0.63704 0.61356 \n",
      " 34:   2.1s   T: 0.55623 0.71477   V: 0.61109 0.64068 \n",
      " 35:   2.1s   T: 0.54193 0.73658   V: 0.60023 0.66102 best\n",
      " 36:   2.1s   T: 0.54840 0.75000   V: 0.60300 0.66441 \n",
      " 37:   2.1s   T: 0.52041 0.75671   V: 0.61902 0.61695 \n",
      " 38:   2.2s   T: 0.53319 0.73322   V: 0.60612 0.66102 \n",
      " 39:   2.1s   T: 0.52674 0.73993   V: 0.59631 0.70169 best\n",
      " 40:   2.1s   T: 0.51492 0.74664   V: 0.59933 0.70169 \n",
      "Stop training at epoch: 40/40\n",
      "Best model was saved at epoch 39 with loss 0.59631: Models/titanic-ori\n"
     ]
    }
   ],
   "source": [
    "trainer.fit_loader(n_epochs=30, train_data=dataloaders['train'], valid_data=dataloaders['val'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
